# -*- coding: utf-8 -*-
"""Data Analysis of Youtube Channel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IvfVGY_BArvzGEOVn3HBBS_6-FCDwxmN

# Aim
* Data Analysis for my favourite channel "Filmy Geeks".[[1]](https://www.youtube.com/@FilmyGeeks)
* Getting to know Youtube API and how to obtain video data.
* Analyzing video data and verify different common "myths" about what makes a video do well on Youtube, for example:
1. Best Performing Videos based on the views.
2. Worst Performing Videos based on the views.
3. View Distribution per Video.
4. Views influencing Likes and Comments.
5. Does the Tags influence the views Count.
6. Longest Video and shortest video
7. Upload Schedule based on the Weekdays
8. Mostly used words in comment section.
"""

from googleapiclient.discovery import build
import pandas as pd
from IPython.display import JSON

api_key='My Youtube API Key'

channel_id= ['UCqkQzjePM64LtMOWzShh_SQ',]

api_service_name = "youtube"
api_version = "v3"


# Get credentials and create an API client
youtube = build(
    api_service_name, api_version, developerKey=api_key)

request= youtube.channels().list(
      part="snippet,contentDetails,statistics",
      id=','.join(channel_id)
  )
response= request.execute()

JSON(response)

def get_channel_stats(youtube, channel_id):
  all_data=[]

  request= youtube.channels().list(
      part="snippet,contentDetails,statistics",
      id=','.join(channel_id)
  )
  response= request.execute()

  for item in response['items']:
    data= {'channelName':item['snippet']['title'],
           'subscribers': item['statistics']['subscriberCount'],
           'views':item['statistics']['viewCount'],
           'totalViews': item['statistics']['videoCount'],
           'playlistId': item['contentDetails']['relatedPlaylists']['uploads']
    }

    all_data.append(data)

  return(pd.DataFrame(all_data))

channel_stats= get_channel_stats(youtube, channel_id)

channel_stats

request = youtube.playlistItems().list(
  part="snippet,contentDetails",
  id=','.join(channel_id)
)
response = request.execute()

print(response)

playlist_id= 'UUqkQzjePM64LtMOWzShh_SQ'
def get_video_ids(youtube, playlist_id):
   
    request = youtube.playlistItems().list(
                part='contentDetails',
                playlistId = playlist_id,
                maxResults = 50)
    response = request.execute()
    
    video_ids = []
    
    for i in range(len(response['items'])):
        video_ids.append(response['items'][i]['contentDetails']['videoId'])
        
    next_page_token = response.get('nextPageToken')
    more_pages = True
    
    while more_pages:
        if next_page_token is None:
            more_pages = False
        else:
            request = youtube.playlistItems().list(
                        part='contentDetails',
                        playlistId = playlist_id,
                        maxResults = 50,
                        pageToken = next_page_token)
            response = request.execute()
    
            for i in range(len(response['items'])):
                video_ids.append(response['items'][i]['contentDetails']['videoId'])
            
            next_page_token = response.get('nextPageToken')
        
    return video_ids

video_ids= get_video_ids(youtube,playlist_id)

len(video_ids)

def get_video_details(youtube, video_ids):
  all_videos_info= []

  for i in range(0,len(video_ids),50):
    request=youtube.videos().list(part='snippet,contentDetails,Statistics',id=','.join(video_ids[i:i+50]))

    response= request.execute()

    for video in response['items']:
      stats_to_keep= { 'snippet':['channelTitle','title','description','tags','publishedAt'],
                      'statistics':['viewCount','likeCount','favouriteCount','commentCount'],
                      'contentDetails':['duration','definition','caption']
                      }

      video_info={}
      video_info['video_id']=video['id']

      for k in stats_to_keep.keys():
        for v in stats_to_keep[k]:
          try:
            video_info[v]=video[k][v]
          except:
            video_info[v]=None

      all_videos_info.append(video_info)

  return pd.DataFrame(all_videos_info)

video_df= get_video_details(youtube, video_ids)
video_df

def get_comments(youtube, video_ids):
    all_comments = []
    
    for video_id in video_ids:
        try:   
            request = youtube.commentThreads().list(
                part="snippet,replies",
                videoId=video_id
            )
            response = request.execute()
        
            comments_in_video = [comment['snippet']['topLevelComment']['snippet']['textOriginal'] for comment in response['items'][0:10]]
            comments_in_video_info = {'video_id': video_id, 'comments': comments_in_video}

            all_comments.append(comments_in_video_info)
            
        except: 
            # When error occurs - most likely because comments are disabled on a video
            print('Could not get comments for video ' + video_id)
        
    return pd.DataFrame(all_comments)

comments_df= get_comments(youtube, video_ids)
comments_df

comments_df['comments'][0]

"""# Data Preprocessing"""

video_df.isnull().any()

video_df.dtypes

video_df.columns

video_df.columns= video_df.columns.str.lower()

video_df.columns

#Changing dtype of 'viewcount','likecount','commentcount'
numeric_cols= ['viewcount','likecount','commentcount']
video_df[numeric_cols]= video_df[numeric_cols].apply(pd.to_numeric,errors='coerce',axis=1)

#Published Date into Day
from dateutil import parser
video_df['publishedat'] = video_df['publishedat'].apply(lambda x: parser.parse(x))
video_df['publishDayName'] = video_df['publishedat'].apply(lambda x: x.strftime("%A"))

!pip install isodate

#Duration into Duration in seconds
from isodate import parse_duration

video_df['durationSecs']= video_df['duration'].apply(lambda x: parse_duration(x))
video_df['durationSecs']=video_df['durationSecs'].astype('timedelta64[s]')

video_df[['durationSecs','duration']]

#Adding Tag count

video_df['tagcount']= video_df['tags'].apply(lambda x: 0 if x is None else len(x))

video_df[['tags','tagcount']]

video_df.head()

video_df.dtypes

video_df.columns

video_df.definition.unique()

final_df= video_df.drop(['video_id','channeltitle','description','tags','publishedat','caption','favouritecount','duration','definition'],axis=1)

final_df.head()

final_df.title[0]

df= final_df.copy()

import string

english_and_spaces= set(string.ascii_letters + string.punctuation + '¿¡ '+string.digits)

df['modified_title']= df['title'].apply(lambda x: ''.join((c for c in x if c in english_and_spaces)))
df['modified_title'] = df['modified_title'].str.replace('| Filmy Geeks', '', regex=True)
df['modified_title'] = df['modified_title'].str.strip()

df['modified_title'][0]

df.head()

"""# EDA"""

import seaborn as sns
import matplotlib.pyplot as plt

"""Best Performing Videos"""

ax= sns.barplot(x='modified_title',y='viewcount',data= df.sort_values('viewcount',ascending=False)[0:9])
plot = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

"""Worst Performing Videos"""

ax= sns.barplot(x='modified_title',y='viewcount',data= df.sort_values('viewcount',ascending=True)[0:5])
plot = ax.set_xticklabels(ax.get_xticklabels(), rotation=90)

"""View Distribution per Video"""

sns.violinplot(video_df['channeltitle'], df['viewcount'])

sns.boxplot(video_df['channeltitle'], df['viewcount'])

"""Views vs. likes & comments"""

fig, ax= plt.subplots(1,2)
sns.scatterplot(data=df, x='commentcount',y='viewcount',ax=ax[0])
sns.scatterplot(data=df, x='likecount',y='viewcount',ax=ax[1])

"""No of tags vs Views"""

sns.scatterplot(data = df, x = "tagcount", y = "viewcount")

"""Video Duration"""

sns.histplot(data=df, x='durationSecs',bins=10)

"""Longest Video & Shortest Video"""

max(df['durationSecs']), min(df['durationSecs'])

df[df.durationSecs==max(df['durationSecs'])]['title']

df[df.durationSecs==min(df['durationSecs'])]['title']

"""Upload Schedule"""

day_df = pd.DataFrame(df['publishDayName'].value_counts())
weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
day_df = day_df.reindex(weekdays)

fig, ax = plt.subplots(figsize=(10, 6))  # Increase the figure size
ax = day_df.reset_index().plot.bar(x='index', y='publishDayName', rot=0, ax=ax)
ax.set_xlabel('Day of the week')
ax.set_ylabel('Number of videos')
ax.set_title('Number of videos published by day of the week')
plt.show()

"""Mostly used words in comment sections"""

# NLP libraries
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt')
from wordcloud import WordCloud

stop_words = set(stopwords.words('english'))
comments_df['comments_no_stopwords'] = comments_df['comments'].apply(lambda x: [item for item in str(x).split() if item not in stop_words])

all_words = list([a for b in comments_df['comments_no_stopwords'].tolist() for a in b])
all_words_str = ' '.join(all_words)
wordcloud = WordCloud(width=2000, height=1000, random_state=1, background_color='black', colormap='viridis', collocations=False).generate(all_words_str)

plt.figure(figsize=(20,10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()